{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Which task do you want to execute\n",
      "\n",
      "1)To get the top 10 relevant documents for your given query\n",
      "\n",
      "2)To get the  5 suggestions for the given query\n",
      "\n",
      "3)Quit\n",
      "1\n",
      "-----------Please Enter Your Phrasal Query that you want here--------------\n",
      "upgrad is awesome\n",
      "The query in vector form is {'upgrad': 1, 'awesom': 1}\n",
      "\n",
      ".........................TASK 1...........relevant documents................wait....for.....a...few..seconds......\n",
      "\n",
      "-----------------------------------------------------------------------------\n",
      "headlines    Astronaut shares 'burrito of awesomeness' view...\n",
      "text         American astronaut Jack David Fischer, who is ...\n",
      "Name: 77887, dtype: object\n",
      "==============================================================================\n",
      "\n",
      "-----------------------------------------------------------------------------\n",
      "headlines    Priyanka is super awesome: Rumoured boyfriend ...\n",
      "text         Priyanka Chopra's rumoured boyfriend, American...\n",
      "Name: 38265, dtype: object\n",
      "==============================================================================\n",
      "\n",
      "-----------------------------------------------------------------------------\n",
      "headlines    Virat Kohli is awesome, says KXIP owner Preity...\n",
      "text         After a Twitter user asked KXIP owner Preity Z...\n",
      "Name: 30006, dtype: object\n",
      "==============================================================================\n",
      "\n",
      "-----------------------------------------------------------------------------\n",
      "headlines    No one quits because everything's awesome: Ins...\n",
      "text         On being asked about his exit from parent comp...\n",
      "Name: 13384, dtype: object\n",
      "==============================================================================\n",
      "\n",
      "-----------------------------------------------------------------------------\n",
      "headlines    Sharing thoughts with young crew is awesome: A...\n",
      "text         Actor Amitabh Bachchan, on social media, wrote...\n",
      "Name: 1403, dtype: object\n",
      "==============================================================================\n",
      "\n",
      "-----------------------------------------------------------------------------\n",
      "headlines    UpGrad has helped learners receive salary hike...\n",
      "text         Over the last 2 years with 460+ transitions, U...\n",
      "Name: 1065, dtype: object\n",
      "==============================================================================\n",
      "\n",
      "-----------------------------------------------------------------------------\n",
      "headlines    upGrad learner switches to career in ML & Al w...\n",
      "text         Saurav Kant, an alumnus of upGrad and IIIT-B's...\n",
      "Name: 0, dtype: object\n",
      "==============================================================================\n",
      "\n",
      "-----------------------------------------------------------------------------\n",
      "headlines    OnePlus unveils Assured Upgrade plan for OnePl...\n",
      "text         As part of OnePlus' Assured Upgrade program, O...\n",
      "Name: 1413, dtype: object\n",
      "==============================================================================\n",
      "\n",
      "-----------------------------------------------------------------------------\n",
      "headlines    UpGrad learner gets a dual transition with a 2...\n",
      "text         Aishwarya Ramachandran, a learner of UpGrad an...\n",
      "Name: 1949, dtype: object\n",
      "==============================================================================\n",
      "\n",
      "-----------------------------------------------------------------------------\n",
      "headlines    UpGrad & IIIT B's Data Science program sees 90...\n",
      "text         UpGrad and IIIT-Bangalore's PG program in Data...\n",
      "Name: 2888, dtype: object\n",
      "==============================================================================\n",
      "\n",
      "Which task do you want to execute\n",
      "\n",
      "1)To get the top 10 relevant documents for your given query\n",
      "\n",
      "2)To get the  5 suggestions for the given query\n",
      "\n",
      "3)Quit\n",
      "1\n",
      "-----------Please Enter Your Phrasal Query that you want here--------------\n",
      "what is upgrad\n",
      "The query in vector form is {'upgrad': 1}\n",
      "\n",
      ".........................TASK 1...........relevant documents................wait....for.....a...few..seconds......\n",
      "\n",
      "-----------------------------------------------------------------------------\n",
      "headlines    BCCI upgrades Jadeja, Pujara to Grade A contracts\n",
      "text         Following their recent performances, the BCCI ...\n",
      "Name: 97272, dtype: object\n",
      "==============================================================================\n",
      "\n",
      "-----------------------------------------------------------------------------\n",
      "headlines    Govt school upgraded after 5-day hunger strike...\n",
      "text         The district administration in Rewari, Haryana...\n",
      "Name: 88580, dtype: object\n",
      "==============================================================================\n",
      "\n",
      "-----------------------------------------------------------------------------\n",
      "headlines    Railways allots â¹50 lakh each to upgrade trains\n",
      "text         The Indian Railways has allotted â¹50 lakh ea...\n",
      "Name: 86070, dtype: object\n",
      "==============================================================================\n",
      "\n",
      "-----------------------------------------------------------------------------\n",
      "headlines    After water breach, 'Doomsday' vault to get â...\n",
      "text         Norway's Global Seed Vault, meant to preserve ...\n",
      "Name: 85194, dtype: object\n",
      "==============================================================================\n",
      "\n",
      "-----------------------------------------------------------------------------\n",
      "headlines    MPs ask for flight seats to be upgraded to Bus...\n",
      "text         Members of the Parliament have proposed that d...\n",
      "Name: 78524, dtype: object\n",
      "==============================================================================\n",
      "\n",
      "-----------------------------------------------------------------------------\n",
      "headlines    Trump on 17-day vacation over cooling system u...\n",
      "text         US President Donald Trump set off on a 17-day ...\n",
      "Name: 77053, dtype: object\n",
      "==============================================================================\n",
      "\n",
      "-----------------------------------------------------------------------------\n",
      "headlines    Govt to spend billions on upgrade of tanks: Re...\n",
      "text         The Defence Ministry has reportedly approved p...\n",
      "Name: 76165, dtype: object\n",
      "==============================================================================\n",
      "\n",
      "-----------------------------------------------------------------------------\n",
      "headlines    Apple upgrades Apple TV to 4K HDR with live sp...\n",
      "text         Apple on Tuesday announced the upgrade for App...\n",
      "Name: 70410, dtype: object\n",
      "==============================================================================\n",
      "\n",
      "-----------------------------------------------------------------------------\n",
      "headlines    Govt approves â¹25,000 crore scheme to upgrad...\n",
      "text         The government on Wednesday approved a â¹25,0...\n",
      "Name: 68127, dtype: object\n",
      "==============================================================================\n",
      "\n",
      "-----------------------------------------------------------------------------\n",
      "headlines    Navy to upgrade military infrastructure in Ind...\n",
      "text         Indian Navy Chief Admiral Sunil Lanba gave app...\n",
      "Name: 64283, dtype: object\n",
      "==============================================================================\n",
      "\n",
      "Which task do you want to execute\n",
      "\n",
      "1)To get the top 10 relevant documents for your given query\n",
      "\n",
      "2)To get the  5 suggestions for the given query\n",
      "\n",
      "3)Quit\n",
      "2\n",
      "-----------Please Enter Your Phrasal Query that you want here--------------\n",
      "upgrad is\n",
      "The query in vector form is {'upgrad': 1}\n",
      "\n",
      "----------The suggestions for our query are---------------\n",
      "\n",
      "upgrad is awesome\n",
      "what is upgrad\n",
      "BCCI upgrades Jadeja, Pujara to Grade A contracts\n",
      "\n",
      "\n",
      "Govt school upgraded after 5-day hunger strike by students\n",
      "\n",
      "\n",
      "Railways allots â¹50 lakh each to upgrade trains\n",
      "\n",
      "\n",
      "After water breach, 'Doomsday' vault to get â¹28 cr upgrade\n",
      "\n",
      "\n",
      "MPs ask for flight seats to be upgraded to Business Class\n",
      "\n",
      "\n",
      "Trump on 17-day vacation over cooling system upgrade\n",
      "\n",
      "\n",
      "Govt to spend billions on upgrade of tanks: Report\n",
      "\n",
      "\n",
      "Apple upgrades Apple TV to 4K HDR with live sports tab\n",
      "\n",
      "\n",
      "Govt approves â¹25,000 crore scheme to upgrade police force\n",
      "\n",
      "\n",
      "Navy to upgrade military infrastructure in Indian Ocean\n",
      "\n",
      "\n",
      "Which task do you want to execute\n",
      "\n",
      "1)To get the top 10 relevant documents for your given query\n",
      "\n",
      "2)To get the  suggestions for the given query\n",
      "\n",
      "3)Quit\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import string\n",
    "import time\n",
    "import math\n",
    "import pickle\n",
    "import sys\n",
    "\n",
    "\n",
    "\n",
    "# returns the dot product of two documents \n",
    "def dotProduct(D1, D2): \n",
    "    Sum = 0.0\n",
    "    for key in D1: \n",
    "        if key in D2: \n",
    "            Sum += (D1[key] * D2[key]) \n",
    "    return Sum\n",
    "\n",
    "# returns the angle in radians \n",
    "# between document vectors \n",
    "def vector_angle(D1, D2): \n",
    "    numerator = dotProduct(D1, D2) \n",
    "    denominator = math.sqrt(dotProduct(D1, D1)*dotProduct(D2, D2)) \n",
    "    return math.acos(numerator / denominator) \n",
    "\n",
    "\n",
    "def dictionary(query):\n",
    "    tokens = tokenise(query)\n",
    "    token_filter = rem_stop_words(tokens)\n",
    "    stemmed_words = stemmed_tokens(token_filter)\n",
    "    token_filter1 = rem_stop_words(stemmed_words)\n",
    "    return token_filter1\n",
    "\n",
    "def make_vector(query):\n",
    "    tokens = dictionary(query)\n",
    "    new_vector = {}\n",
    "    for token in tokens:\n",
    "        new_vector[token.lower()] = query.count(token)\n",
    "    return new_vector\n",
    "\n",
    "def log_term_frequency(frequency):\n",
    "    if frequency > 0:\n",
    "        return (1+math.log(frequency))\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def inverse_document(df,N):\n",
    "    return math.log(N/df)\n",
    "\n",
    "def weight_tf_idf(tf,df,N):\n",
    "    return log_term_frequency(tf)*inverse_document(df,N)\n",
    "\n",
    "def score_sort(score_card,top_10_results):\n",
    "    flag = 0\n",
    "    if top_10_results:\n",
    "        if top_10_results[0][1]<=score_card[1]:\n",
    "            top_10_results.insert(0,score_card)\n",
    "        elif top_10_results[-1][1]>=score_card[1]:\n",
    "            top_10_results.append(score_card)\n",
    "        else:\n",
    "            for i in range(len(top_10_results)-1):\n",
    "                if top_10_results[i][1]>=score_card[1]>=top_10_results[i+1][1]:\n",
    "                    top_10_results.insert(i+1,score_card)\n",
    "                    flag = 1\n",
    "                    break \n",
    "    else:\n",
    "        top_10_results.append(score_card)\n",
    "    return top_10_results[:10]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def frequency_count(tokens):\n",
    "    frequency_dictionary={}\n",
    "    for token in tokens:\n",
    "        if token not in frequency_dictionary:\n",
    "            frequency_dictionary[token]=0\n",
    "        frequency_dictionary[token]+=1\n",
    "    return frequency_dictionary\n",
    "\n",
    "def frequency_words(data):\n",
    "    tokens = []\n",
    "    for token_list in data.values():\n",
    "        tokens = tokens + token_list\n",
    "#     print(len(tokens))\n",
    "    fdist = frequency_count(tokens)\n",
    "#     for i in fdist.items():\n",
    "#         print(i)\n",
    "#     print(len(fdist.values()))\n",
    "    return list(fdist.keys())\n",
    "\n",
    "\n",
    "def inverted_index(preprocessed_data):\n",
    "    words = frequency_words(preprocessed_data)\n",
    "    index = {}\n",
    "    for word in words:\n",
    "        for doc, tokens in preprocessed_data.items():\n",
    "            if word in tokens :\n",
    "                if word in index.keys():\n",
    "                    index[word].append(doc)\n",
    "                else:\n",
    "                    index[word] = [doc]\n",
    "    return index\n",
    "\n",
    "\n",
    "def vowel_or_consonant(word,i):\n",
    "    if word[i] in [\"a\",\"e\",\"i\",\"o\",\"u\"]:\n",
    "        return True\n",
    "    if i-1>0:\n",
    "        if ((word[i]==\"y\") and (word[i-1] not in [\"a\",\"e\",\"i\",\"o\",\"u\"])):\n",
    "             return True\n",
    "    return False\n",
    "\n",
    "def double_consonants(word):\n",
    "    if len(word)>2:\n",
    "        if not ((vowel_or_consonant(word,-1) or (vowel_or_consonant(word,-2)))):\n",
    "                  return True\n",
    "    return False\n",
    "\n",
    "def o_form(word):\n",
    "    if len(word)>=3:\n",
    "        if ((not (vowel_or_consonant(word,-1) or (vowel_or_consonant(word,-3)))) and(vowel_or_consonant(word,-2))):\n",
    "            if word[-1] not in [\"w\",\"x\",\"y\"]:\n",
    "                  return True\n",
    "    return False\n",
    "\n",
    "def check_vc(word):\n",
    "    required_string=\"\"\n",
    "    list_vc = []\n",
    "    for i in range(len(word)):\n",
    "        if vowel_or_consonant(word,i):\n",
    "            if i!=0:\n",
    "                previous = list_vc[-1]\n",
    "                if previous!='V':\n",
    "                    list_vc.append('V')\n",
    "            else:\n",
    "                list_vc.append('V')\n",
    "        else:\n",
    "            if i!=0:\n",
    "                previous = list_vc[-1]\n",
    "                if previous !=\"C\":\n",
    "                    list_vc.append(\"C\")\n",
    "            else:\n",
    "                list_vc.append(\"V\")\n",
    "\n",
    "    for j in list_vc:\n",
    "        required_string+=j\n",
    "    return required_string\n",
    "\n",
    "def word_vowel_check(word):\n",
    "    for i in range(len(word)):\n",
    "        if vowel_or_consonant(word,i):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def check_mvalue(word):\n",
    "    vc = check_vc(word)\n",
    "    return vc.count(\"VC\")\n",
    "\n",
    "def check_m_0_replace(word,remain,attach):\n",
    "    x = word.rfind(remain)\n",
    "    if check_mvalue(word[0:x])>0:\n",
    "        return word[0:x]+attach\n",
    "    else:\n",
    "        return word\n",
    "\n",
    "def check_m_1_replace(word,remain,attach):\n",
    "    x = word.rfind(remain)\n",
    "    if check_mvalue(word[0:x])>1:\n",
    "        return word[0:x]+attach\n",
    "    else:\n",
    "        return word\n",
    "\n",
    "\n",
    "def step1a(word):\n",
    "    if word.endswith('sses'):\n",
    "        word = word.replace('sses','ss')\n",
    "    elif word.endswith('ies'):\n",
    "        word = word.replace('ies','i')\n",
    "    elif word.endswith('ss'):\n",
    "         word = word.replace('ss','ss')\n",
    "    elif word.endswith('s'):\n",
    "        word = word.replace('s', \"\")\n",
    "    return word\n",
    "\n",
    "def step1b(word):\n",
    "    flag = False\n",
    "\n",
    "    if word.endswith(\"eed\"):\n",
    "        if check_mvalue(word[0:-3])>0:\n",
    "            word = word[0:-3]+\"ee\"\n",
    "    elif word.endswith('ed'):\n",
    "        if word_vowel_check(word[0:-2]):\n",
    "            word = word[0:-2]\n",
    "            flag = True\n",
    "    elif word.endswith(\"ing\"):\n",
    "        if word_vowel_check(word[0:-3]):\n",
    "            word = word[0:-3]\n",
    "            flag = True\n",
    "\n",
    "    if flag==True:\n",
    "        if (word.endswith('at') or word.endswith('bl') or word.endswith('iz')):\n",
    "            word = word + \"e\"\n",
    "        elif double_consonants(word) and not word.endswith('l') and not word.endswith('s') and not word.endswith('z'):\n",
    "            word = word[:-1]\n",
    "        elif check_mvalue(word)==1 and o_form(word):\n",
    "            word = word + \"e\"\n",
    "    return word\n",
    "\n",
    "def step1c(word):\n",
    "    if word.endswith(\"y\"):\n",
    "        if word_vowel_check(word[0:-1]):\n",
    "            word = word[0:-1]+\"i\"\n",
    "    return word\n",
    "\n",
    "def step2(word):\n",
    "    if word.endswith(\"ational\"):\n",
    "        word = check_m_0_replace(word,\"ational\",\"ate\")\n",
    "    elif word.endswith(\"tional\"):\n",
    "        word = check_m_0_replace(word,\"tional\",\"tion\")\n",
    "    elif word.endswith(\"enci\"):\n",
    "        word = check_m_0_replace(word,\"enci\",\"ence\")\n",
    "    elif word.endswith(\"anci\"):\n",
    "        word = check_m_0_replace(word,\"anci\",\"ance\")\n",
    "    elif word.endswith(\"izer\"):\n",
    "        word = check_m_0_replace(word,\"izer\",\"ize\")\n",
    "    elif word.endswith(\"abli\"):\n",
    "        word = check_m_0_replace(word,\"abli\",\"able\")\n",
    "    elif word.endswith(\"alli\"):\n",
    "        word = check_m_0_replace(word,\"alli\",\"al\")\n",
    "    elif word.endswith(\"entli\"):\n",
    "        word = check_m_0_replace(word,\"entli\",\"ent\")\n",
    "    elif word.endswith(\"eli\"):\n",
    "        word = check_m_0_replace(word,\"eli\",\"e\")\n",
    "    elif word.endswith(\"ousli\"):\n",
    "        word = check_m_0_replace(word,\"ousli\",\"ous\")\n",
    "    elif word.endswith(\"ization\"):\n",
    "        word = check_m_0_replace(word,\"ization\",\"ize\")\n",
    "    elif word.endswith(\"ation\"):\n",
    "        word = check_m_0_replace(word,\"ation\",\"ate\")\n",
    "    elif word.endswith(\"ator\"):\n",
    "        word = check_m_0_replace(word,\"ator\",\"ate\")\n",
    "    elif word.endswith(\"alism\"):\n",
    "        word = check_m_0_replace(word,\"alism\",\"al\")\n",
    "    elif word.endswith(\"iveness\"):\n",
    "        word = check_m_0_replace(word,\"iveness\",\"ive\")\n",
    "    elif word.endswith(\"fulness\"):\n",
    "        word = check_m_0_replace(word,\"fulness\",\"ful\")\n",
    "    elif word.endswith(\"ousness\"):\n",
    "        word = check_m_0_replace(word,\"ousness\",\"ous\")\n",
    "    elif word.endswith(\"aliti\"):\n",
    "        word = check_m_0_replace(word,\"aliti\",\"al\")\n",
    "    elif word.endswith(\"iviti\"):\n",
    "        word = check_m_0_replace(word,\"iviti\",\"ive\")\n",
    "    elif word.endswith(\"biliti\"):\n",
    "        word = check_m_0_replace(word,\"biliti\",\"ble\")\n",
    "\n",
    "    return word\n",
    "\n",
    "def step3(word):\n",
    "    if word.endswith(\"icate\"):\n",
    "        word = check_m_0_replace(word,\"icate\",\"ic\")\n",
    "    elif word.endswith(\"ative\"):\n",
    "        word = check_m_0_replace(word,\"ative\",\"\")\n",
    "    elif word.endswith(\"alize\"):\n",
    "        word = check_m_0_replace(word,\"alize\",\"al\")\n",
    "    elif word.endswith(\"iciti\"):\n",
    "        word = check_m_0_replace(word,\"iciti\",\"ic\")\n",
    "    elif word.endswith(\"ful\"):\n",
    "        word = check_m_0_replace(word,\"ful\",\"\")\n",
    "    elif word.endswith(\"ness\"):\n",
    "        word = check_m_0_replace(word,\"ness\",\"\")\n",
    "    elif word.endswith(\"ical\"):\n",
    "        word = check_m_0_replace(word,\"ical\",\"ic\")\n",
    "    return word\n",
    "\n",
    "def step4(word):\n",
    "    if word.endswith(\"al\"):\n",
    "        word = check_m_1_replace(word,\"al\",\"\")\n",
    "    elif word.endswith(\"ance\"):\n",
    "        word = check_m_1_replace(word,\"ance\",\"\")\n",
    "    elif word.endswith(\"ence\"):\n",
    "        word = check_m_1_replace(word,\"ence\",\"\")\n",
    "    elif word.endswith(\"er\"):\n",
    "        word = check_m_1_replace(word,\"er\",\"\")\n",
    "    elif word.endswith(\"ic\"):\n",
    "        word = check_m_1_replace(word,\"ic\",\"\")\n",
    "    elif word.endswith(\"able\"):\n",
    "        word = check_m_1_replace(word,\"able\",\"\")\n",
    "    elif word.endswith(\"ible\"):\n",
    "        word = check_m_1_replace(word,\"ible\",\"\")\n",
    "    elif word.endswith(\"ant\"):\n",
    "        word = check_m_1_replace(word,\"ant\",\"\")\n",
    "    elif word.endswith(\"ement\"):\n",
    "        word = check_m_1_replace(word,\"ement\",\"\")\n",
    "    elif word.endswith(\"ment\"):\n",
    "        word = check_m_1_replace(word,\"ment\",\"\")\n",
    "    elif word.endswith(\"ent\"):\n",
    "        word = check_m_1_replace(word,\"ent\",\"\")\n",
    "    elif word.endswith(\"ou\"):\n",
    "        word = check_m_1_replace(word,\"ou\",\"\")\n",
    "    elif word.endswith(\"ism\"):\n",
    "        word = check_m_1_replace(word,\"ism\",\"\")\n",
    "    elif word.endswith(\"ate\"):\n",
    "        word = check_m_1_replace(word,\"ate\",\"\")\n",
    "    elif word.endswith(\"iti\"):\n",
    "        word = check_m_1_replace(word,\"iti\",\"\")\n",
    "    elif word.endswith(\"ous\"):\n",
    "        word = check_m_1_replace(word,\"ous\",\"\")\n",
    "    elif word.endswith(\"ive\"):\n",
    "        word = check_m_1_replace(word,\"ive\",\"\")\n",
    "    elif word.endswith(\"ize\"):\n",
    "        word = check_m_1_replace(word,\"ize\",\"\")\n",
    "    elif word.endswith(\"ion\"):\n",
    "        if check_mvalue(word[0:-3])>1 and ((word[0:-3]).endswith(\"s\") or (word[0:-3]).endswith(\"t\")):\n",
    "            word = word[0:-3]\n",
    "    return word\n",
    "\n",
    "def step5a(word):\n",
    "    if word.endswith(\"e\"):\n",
    "        if check_mvalue(word[0:-1])>1:\n",
    "            word = word[0:-1]\n",
    "        elif check_mvalue(word[0:-1])==1 and not o_form(word[0:-1]):\n",
    "            word = word[0:-1]\n",
    "    return word\n",
    "\n",
    "def step5b(word):\n",
    "    if check_mvalue(word)>1 and double_consonants(word) and word.endswith(\"l\"):\n",
    "        word = word[0:-1]\n",
    "    return word\n",
    "\n",
    "\n",
    "\n",
    "def stemming(word):\n",
    "        word = step1a(word)\n",
    "        word = step1b(word)\n",
    "        word = step1c(word)\n",
    "        word = step2(word)\n",
    "        word = step3(word)\n",
    "        word = step4(word)\n",
    "        word = step5a(word)\n",
    "        word = step5b(word)\n",
    "        return word\n",
    "\n",
    "\n",
    "\n",
    "def stemmed_tokens(token_filter):\n",
    "#     creating stemming without using nltk\n",
    "    stemmed_words = [stemming(token) for token in token_filter]\n",
    "    return stemmed_words\n",
    "\n",
    "\n",
    "def rem_stop_words(tokens):\n",
    "#     remoing the stop words\n",
    "    stop_words=['ourselves', 'hers', 'between', 'yourself', 'but', 'again', 'there',\n",
    "                'about','once','during','out','very','having','with', 'they', 'own',\n",
    "                'an', 'be', 'some', 'for', 'do', 'its','yours','such','into','of','most',\n",
    "                'itself','other','off','is','s','am','or','who','as','from','him','each',\n",
    "                'the','themselves','until','below','are','we','these','your','his','through',\n",
    "                'don','nor','me','were','her','more','himself','this', 'down','should',\n",
    "                'our','their','while', 'above', 'both', 'up', 'to', 'ours', 'had', 'she',\n",
    "                'all','no','when','at','any','before','them','same','and','been','have',\n",
    "                'in','will','on','does','yourselves','then','that','because','what','over',\n",
    "                'why', 'so','can','did', 'not','now', 'under','he','you','herself','has',\n",
    "                'just','where','too','only','myself','which','those','i','after', 'few',\n",
    "                'whom', 't', 'being','if', 'theirs', 'my','against','a','by','doing', 'it',\n",
    "                'how', 'further', 'was', 'here', 'than']\n",
    "    token_filter = [token for token in tokens if token not in stop_words and len(token) > 2]\n",
    "    return token_filter\n",
    "\n",
    "def tokenise(content):\n",
    "#     tokenisation like removing punchuation and other etc.\n",
    "    remove_punctuation = str.maketrans(\"\",\"\",string.punctuation)\n",
    "    modified_data = content.translate(remove_punctuation)\n",
    "    modified_data = ''.join([i for i in modified_data if not i.isdigit()])\n",
    "    x=modified_data.strip()\n",
    "    z=x.split()\n",
    "    return z\n",
    "\n",
    "\n",
    "def preprocess_data(data):\n",
    "    term_dictionary = {}\n",
    "    for i in range(len(data)):\n",
    "        tokens = tokenise(data.loc[i][\"headlines\"])\n",
    "        token_filter = rem_stop_words(tokens)\n",
    "        stemmed_words = stemmed_tokens(token_filter)\n",
    "        token_filter1 = rem_stop_words(stemmed_words)\n",
    "        term_dictionary[i]=token_filter1\n",
    "    return term_dictionary\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "    \n",
    "        news_summary_more_ds = pd.read_csv(\"./news_summary_more.csv\")\n",
    "# -------------------loading and storing the created dictionary in pickle file---------------------------------------\n",
    "#     preprocessed_data = preprocess_data(news_summary_more_ds)\n",
    "#     invertedindex = inverted_index(preprocessed_data)\n",
    "#     pickle_out = open(\"dict.pickle\",\"wb\")\n",
    "#     pickle.dump(invertedindex, pickle_out)\n",
    "#     pickle_out.close()\n",
    "# ---------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# ----------------------loading the inverted index using pickle that is created by using above steps-------------------\n",
    "        print(\"Which task do you want to execute\\n\")\n",
    "        print(\"1)To get the top 10 relevant documents for your given query\\n\")\n",
    "        print(\"2)To get the  5 suggestions for the given query\\n\")\n",
    "        print(\"3)Quit\")\n",
    "        p = int(input())\n",
    "        \n",
    "        pickle_in = open(\"dict.pickle\",\"rb\")\n",
    "        invertedindex = pickle.load(pickle_in)\n",
    "        \n",
    "        Total_documents = 98401\n",
    "        suggestions = []\n",
    "        \n",
    "        while p!=0:\n",
    "            top_10_results = []\n",
    "            query = input(\"-----------Please Enter Your Phrasal Query that you want here--------------\\n\")\n",
    "            query_vector = make_vector(query)\n",
    "            print(\"The query in vector form is {}\\n\".format(query_vector))\n",
    "            if p==1:\n",
    "                print('.........................TASK 1...........relevant documents................wait....for.....a...few..seconds......\\n')\n",
    "                for i in range(len(news_summary_more_ds)):\n",
    "                    documents_list = news_summary_more_ds.loc[i]['headlines'] \n",
    "                    doc_vector = make_vector(documents_list)\n",
    "                    search_tokens = set(query_vector.keys()).intersection(set(doc_vector.keys()))\n",
    "                    if search_tokens:\n",
    "                        score = 0\n",
    "                    for token in search_tokens:\n",
    "                        score += weight_tf_idf(query_vector[token],len(invertedindex[token]),Total_documents)*weight_tf_idf(doc_vector[token],len(invertedindex[token]),Total_documents)\n",
    "                        rownumber = i\n",
    "                        score_card = [rownumber,score]\n",
    "                        top_10_results = score_sort(score_card,top_10_results)\n",
    "        \n",
    "        \n",
    "                if top_10_results:\n",
    "                    for i in range(len(top_10_results)):\n",
    "                        x = top_10_results[i][0]\n",
    "                        print('-----------------------------------------------------------------------------')\n",
    "                        print(news_summary_more_ds.loc[x])\n",
    "                        print('==============================================================================\\n')\n",
    "                else:\n",
    "                    print('No Search Results Found')\n",
    "                    \n",
    "                suggestions.append(query)\n",
    "                print(\"Which task do you want to execute\\n\")\n",
    "                print(\"1)To get the top 10 relevant documents for your given query\\n\")\n",
    "                print(\"2)To get the  5 suggestions for the given query\\n\")\n",
    "                print(\"3)Quit\")\n",
    "                p = int(input())\n",
    "            elif p==2:\n",
    "                l = []\n",
    "#                 print(suggestions)\n",
    "                if len(suggestions)>0:\n",
    "                    for i in suggestions:\n",
    "                        z = make_vector(i)\n",
    "                        angle = dotProduct(z,query_vector)\n",
    "                        if angle > 0:\n",
    "                            l.append(i)\n",
    "            \n",
    "                for i in range(len(news_summary_more_ds)):\n",
    "                    documents_list = news_summary_more_ds.loc[i]['headlines'] \n",
    "                    doc_vector = make_vector(documents_list)\n",
    "                    search_tokens = set(query_vector.keys()).intersection(set(doc_vector.keys()))\n",
    "                    if search_tokens:\n",
    "                        score = 0\n",
    "                    for token in search_tokens:\n",
    "                        score += weight_tf_idf(query_vector[token],len(invertedindex[token]),Total_documents)*weight_tf_idf(doc_vector[token],len(invertedindex[token]),Total_documents)\n",
    "                        rownumber = i\n",
    "                        score_card = [rownumber,score]\n",
    "                        top_10_results = score_sort(score_card,top_10_results)\n",
    "                        \n",
    "                print(\"----------The suggestions for our query are---------------\\n\")\n",
    "                if len(l)>0:\n",
    "                    for i in l:\n",
    "                        print(i)\n",
    "                if top_10_results:\n",
    "                    for i in range(len(top_10_results)):\n",
    "                        x = top_10_results[i][0]\n",
    "                        print(news_summary_more_ds.loc[x]['headlines'])\n",
    "                        print(\"\\n\")\n",
    "                elif len(l)==0:\n",
    "                    print('No Search Results Found\\n')\n",
    "                \n",
    "                print(\"Which task do you want to execute\\n\")\n",
    "                print(\"1)To get the top 10 relevant documents for your given query\\n\")\n",
    "                print(\"2)To get the  suggestions for the given query\\n\")\n",
    "                print(\"3)Quit\")\n",
    "                p = int(input())\n",
    "            else:\n",
    "                p=0\n",
    "                \n",
    "        print(\"..........you are quit from the search session............\")\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
